# main.py
import asyncio
import json
from typing import List, Dict
from config import ConfigLoader
from llm_client import query_target_llm
from evaluator import evaluate_jailbreak
from reporter import generate_report

# é…ç½®
PROMPTS_FILE = "prompts.json"
LLM_CONFIGS_FILE = "llm_configs.json"
RESULTS_FILE = "results.json"


async def evaluate_single_case(prompt_data: Dict, llm_config: Dict) -> Dict:
    """
    å¯¹å•ä¸ª prompt + å•ä¸ª LLM è¿›è¡Œè¶Šç‹±æ£€æµ‹
    :param prompt_data: åŒ…å« prompt, category, description çš„å­—å…¸
    :param llm_config: LLM é…ç½®
    :return: æµ‹è¯•ç»“æœ
    """
    prompt_text = prompt_data["prompt"]
    print(f"\nğŸ” æµ‹è¯• LLM: {llm_config['name']} | ç±»å‹: {prompt_data['category']} | æç¤º: {prompt_text[:50]}...")

    # 1. æŸ¥è¯¢ç›®æ ‡æ¨¡å‹
    response = await query_target_llm(prompt_text, llm_config)

    # 2. è¶Šç‹±è¯„ä¼°
    eval_result = await evaluate_jailbreak(prompt_text, response)

    # 3. è¿”å›ç»“æ„åŒ–ç»“æœï¼ˆåŒ…å«åˆ†ç±»ä¿¡æ¯ï¼‰
    return {
        "prompt": prompt_text,
        "category": prompt_data["category"],
        "description": prompt_data["description"],
        "llm": llm_config["name"],
        "model": llm_config["model"],
        "response": response,
        "evaluation": eval_result
    }


async def main():
    print("ğŸ›¡ï¸ å¯åŠ¨ LLM è¶Šç‹±æ”»å‡»æ£€æµ‹ç³»ç»Ÿ...")

    try:
        # åŠ è½½é…ç½®
        prompts: List[Dict] = ConfigLoader.load_prompts(PROMPTS_FILE)  # ç°åœ¨æ˜¯ dict åˆ—è¡¨
        llm_configs: List[Dict] = ConfigLoader.load_llm_configs(LLM_CONFIGS_FILE)

        if not prompts:
            print("âŒ æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æç¤ºè¯ï¼Œç¨‹åºé€€å‡ºã€‚")
            return
        if not llm_configs:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„ LLM é…ç½®ï¼Œç¨‹åºé€€å‡ºã€‚")
            return

        # ç”Ÿæˆæ‰€æœ‰æµ‹è¯•ä»»åŠ¡ï¼ˆæ¯ä¸ª LLM Ã— æ¯ä¸ª promptï¼‰
        tasks = [
            evaluate_single_case(prompt_data, llm_config)
            for llm_config in llm_configs
            for prompt_data in prompts
        ]

        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        results = await asyncio.gather(*tasks)

        # ä¿å­˜åŸå§‹ç»“æœ
        with open(RESULTS_FILE, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"âœ… åŸå§‹ç»“æœå·²ä¿å­˜è‡³ {RESULTS_FILE}")

        # âœ… ç”Ÿæˆå¢å¼ºæŠ¥å‘Šï¼ˆå«åˆ†ç±»ä¿¡æ¯ï¼‰
        generate_report(results, json_path="report.json", html_path="report.html")

        # æ±‡æ€»ç»Ÿè®¡
        total = len(results)
        jailbreak_count = sum(1 for r in results if r["evaluation"]["is_jailbreak"])
        success_rate = jailbreak_count / total if total > 0 else 0

        print(f"\nğŸ“ˆ æœ€ç»ˆç»Ÿè®¡")
        print(f"   æ€»æµ‹è¯•æ•°: {total}")
        print(f"   è¶Šç‹±æˆåŠŸ: {jailbreak_count}")
        print(f"   æˆåŠŸç‡: {success_rate:.1%}")
        print(f"âœ… æ‰€æœ‰ç»“æœå·²ä¿å­˜è‡³:")
        print(f"   - {RESULTS_FILE}")
        print(f"   - report.json")
        print(f"   - report.html")

    except Exception as e:
        print(f"ğŸ’¥ ç³»ç»Ÿå¼‚å¸¸ï¼š{e}")
        raise


if __name__ == "__main__":
    asyncio.run(main())